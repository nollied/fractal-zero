{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyllan/miniconda3/envs/fz/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "\n",
    "from fractal_zero.config import FMCConfig\n",
    "from fractal_zero.search.fmc import FMC\n",
    "from fractal_zero.models.prediction import FullyConnectedPredictionModel\n",
    "from fractal_zero.models.policies.cartpole_policy import CartpolePolicy\n",
    "from fractal_zero.vectorized_environment import (\n",
    "    RayVectorizedEnvironment,\n",
    "    VectorizedDynamicsModelEnvironment,\n",
    "\n",
    ")\n",
    "from fractal_zero.trainers.online import OnlineFMCPolicyTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WALKERS = 64\n",
    "\n",
    "class CartpolePolicy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 2),\n",
    "            # torch.nn.Sigmoid(),  # keep MSE from exploding\n",
    "        )\n",
    "\n",
    "    def forward(self, observations, with_randomness: bool = False):\n",
    "        observations = torch.tensor(observations).float()\n",
    "\n",
    "        y = self.net(observations)\n",
    "\n",
    "        if with_randomness:\n",
    "            # center = embeddings.std()\n",
    "            # center = y.var()\n",
    "            # centered_uniform_noise = (torch.rand_like(y) * center) - (center / 2)\n",
    "            # y += centered_uniform_noise\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return y\n",
    "\n",
    "    def parse_action(self, actions):\n",
    "        # actions = torch.where(actions > 0.5, 1, 0).flatten()\n",
    "        actions = actions.argmax(-1)\n",
    "        l = actions.tolist()\n",
    "        return l\n",
    "\n",
    "policy_model = CartpolePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyllan/miniconda3/envs/fz/lib/python3.10/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/dyllan/miniconda3/envs/fz/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/dyllan/miniconda3/envs/fz/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=0.03, weight_decay=1e-4)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "loss_func = torch.nn.functional.cross_entropy\n",
    "policy_trainer = OnlineFMCPolicyTrainer(\"CartPole-v0\", policy_model, optimizer, NUM_WALKERS, loss_spec=loss_func, use_ray=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdyllan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/Users/dyllan/miniconda3/envs/fz/lib/python3.10/site-packages/wandb/sdk/lib/ipython.py:46: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dyllan/Code/fractal-zero/notebooks/wandb/run-20220930_170611-17jo3tzw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dyllan/fz-policy-trainer-game-tree/runs/17jo3tzw\" target=\"_blank\">absurd-planet-49</a></strong> to <a href=\"https://wandb.ai/dyllan/fz-policy-trainer-game-tree\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dyllan/fz-policy-trainer-game-tree/runs/17jo3tzw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc551e2e230>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"fz-policy-trainer-game-tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyllan/Code/fractal-zero/fractal_zero/vectorized_environment.py:176: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  states = self.observation_encoder(observations)\n",
      "/Users/dyllan/miniconda3/envs/fz/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:179: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 199.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 199.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 184.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 195.0\n",
      "best path reward 180.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 200.0\n",
      "best path reward 184.0\n",
      "best path reward 163.0\n",
      "best path reward 200.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "train_steps_per_episode = 10\n",
    "eval_every = 20\n",
    "max_steps = 200\n",
    "\n",
    "best_total_rewards = float(\"-inf\")\n",
    "best_model = None\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    policy_trainer.generate_episode_data(max_steps)\n",
    "    print(\"best path reward\", policy_trainer.fmc.tree.best_path.total_reward)\n",
    "\n",
    "    for i in range(train_steps_per_episode):\n",
    "        policy_trainer.train_on_latest_episode()\n",
    "\n",
    "        if i % eval_every == 0:\n",
    "            total_rewards = policy_trainer.evaluate_policy(max_steps)\n",
    "\n",
    "            if total_rewards > best_total_rewards:\n",
    "                best_total_rewards = total_rewards\n",
    "                best_model = deepcopy(policy_model)\n",
    "                \n",
    "                # torch.save(best_model, \"models/best_cartpole_policy.pth\")\n",
    "\n",
    "    # lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_trainer.fmc.clone_receives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('fz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22a1c3704ac0fb9957d7546d091a04b357e6fc94e87ba29e4248674f63aaffa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
