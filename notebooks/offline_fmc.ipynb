{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "\n",
    "from fractal_zero.config import FMCConfig\n",
    "from fractal_zero.search.fmc import FMC\n",
    "from fractal_zero.models.prediction import FullyConnectedPredictionModel\n",
    "from fractal_zero.models.policies.cartpole_policy import CartpolePolicy\n",
    "from fractal_zero.vectorized_environment import (\n",
    "    RayVectorizedEnvironment,\n",
    "    VectorizedDynamicsModelEnvironment,\n",
    "\n",
    ")\n",
    "from fractal_zero.trainers.online import OfflineFMCPolicyTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WALKERS = 256\n",
    "\n",
    "class CartpolePolicy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 2),\n",
    "            # torch.nn.Sigmoid(),  # keep MSE from exploding\n",
    "        )\n",
    "\n",
    "    def forward(self, observations, with_randomness: bool = False):\n",
    "        observations = torch.tensor(observations).float()\n",
    "\n",
    "        y = self.net(observations)\n",
    "\n",
    "        if with_randomness:\n",
    "            # center = embeddings.std()\n",
    "            # center = y.var()\n",
    "            # centered_uniform_noise = (torch.rand_like(y) * center) - (center / 2)\n",
    "            # y += centered_uniform_noise\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return y\n",
    "\n",
    "    def parse_action(self, actions):\n",
    "        # actions = torch.where(actions > 0.5, 1, 0).flatten()\n",
    "        actions = actions.argmax(-1)\n",
    "        l = actions.tolist()\n",
    "        return l\n",
    "\n",
    "policy_model = CartpolePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=0.01)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "loss_func = torch.nn.functional.cross_entropy\n",
    "policy_trainer = OfflineFMCPolicyTrainer(\"CartPole-v0\", policy_model, optimizer, NUM_WALKERS, loss_spec=loss_func, use_ray=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"fz-policy-trainer-game-tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "train_steps_per_episode = 5\n",
    "eval_every = 20\n",
    "max_steps = 200\n",
    "\n",
    "best_total_rewards = float(\"-inf\")\n",
    "best_model = None\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    policy_trainer.generate_episode_data(max_steps)\n",
    "    print(\"best path reward\", policy_trainer.fmc.tree.best_path.total_reward)\n",
    "\n",
    "    for i in range(train_steps_per_episode):\n",
    "        policy_trainer.train_on_latest_episode()\n",
    "\n",
    "        if i % eval_every == 0:\n",
    "            total_rewards = policy_trainer.evaluate_policy(max_steps)\n",
    "\n",
    "            if total_rewards > best_total_rewards:\n",
    "                best_total_rewards = total_rewards\n",
    "                best_model = deepcopy(policy_model)\n",
    "                \n",
    "                # torch.save(best_model, \"models/best_cartpole_policy.pth\")\n",
    "\n",
    "    # lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_trainer.fmc.clone_receives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('fz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22a1c3704ac0fb9957d7546d091a04b357e6fc94e87ba29e4248674f63aaffa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
